{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32MNb4_xsB7"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX91gkFHtj-5"
      },
      "outputs": [],
      "source": [
        "# Import and/or install libraries\n",
        "\n",
        "import subprocess, os\n",
        "\n",
        "try:\n",
        "    import geemap, ee\n",
        "except ImportError:\n",
        "    subprocess.check_call([\"python\", '-m', 'pip', 'install', '-U', 'geemap'])\n",
        "    import geemap, ee\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd4amEOkxAJ1"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Drive to access files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_SERVICE_ACCOUNT'] = '[gee-1-238@nature-watch-387210.iam.gserviceaccount.com](mailto:gee-1-238@nature-watch-387210.iam.gserviceaccount.com)'\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/drive/MyDrive/keys/nature-watch-keys/nature-watch-gee-1.json'"
      ],
      "metadata": {
        "id": "RCPvwrDKnKwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEtv-EFnvqgQ"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Earth Engine if neccessary\n",
        "\n",
        "service_account = os.environ.get('GOOGLE_SERVICE_ACCOUNT')\n",
        "credentials = ee.ServiceAccountCredentials(service_account, os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'))\n",
        "ee.Initialize(credentials)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Cloud\n",
        "\n",
        "from google.cloud import storage\n",
        "client = storage.Client()"
      ],
      "metadata": {
        "id": "BUzWkq9vbMin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started\n",
        "\n",
        "We will start by using Google Dynamic World and using the built layer. It seems like we should also filter the certainty to be greater than 0.06 (highest false positive in the grassland had a certainty of 0.0569)."
      ],
      "metadata": {
        "id": "rUABCOGM3l0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AOI\n",
        "\n",
        "aoi = ee.Geometry({\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              31.092023309844734,\n",
        "              -24.821258854952376\n",
        "            ],\n",
        "            [\n",
        "              31.092023309844734,\n",
        "              -25.303992405280965\n",
        "            ],\n",
        "            [\n",
        "              31.791037410539502,\n",
        "              -25.303992405280965\n",
        "            ],\n",
        "            [\n",
        "              31.791037410539502,\n",
        "              -24.821258854952376\n",
        "            ],\n",
        "            [\n",
        "              31.092023309844734,\n",
        "              -24.821258854952376\n",
        "            ]\n",
        "          ]\n",
        "        ],\n",
        "      })\n",
        "\n",
        "\n",
        "rock = ee.Geometry({\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              31.284,\n",
        "              -25.360\n",
        "            ],\n",
        "            [\n",
        "              31.285,\n",
        "              -25.360\n",
        "            ],\n",
        "            [\n",
        "              31.285,\n",
        "              -25.361\n",
        "            ],\n",
        "            [\n",
        "              31.284,\n",
        "              -25.361\n",
        "            ],\n",
        "            [\n",
        "              31.284,\n",
        "              -25.360\n",
        "            ],\n",
        "          ]\n",
        "        ],\n",
        "      })\n",
        "\n",
        "\n",
        "town = ee.Geometry({\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              31.14157858391627,\n",
        "              -25.17820869636482\n",
        "            ],\n",
        "            [\n",
        "              31.14157858391627,\n",
        "              -25.180385520009708\n",
        "            ],\n",
        "            [\n",
        "              31.143983964831108,\n",
        "              -25.180385520009708\n",
        "            ],\n",
        "            [\n",
        "              31.143983964831108,\n",
        "              -25.17820869636482\n",
        "            ],\n",
        "            [\n",
        "              31.14157858391627,\n",
        "              -25.17820869636482\n",
        "            ]\n",
        "          ]\n",
        "        ],\n",
        "      })"
      ],
      "metadata": {
        "id": "BfKUbBdm_eKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import all building layers\n",
        "\n",
        "First, we experiment with 2022 for our AOI. Here we import Google Dynamic World which we primarily use in. `best_people` is the important layer going forward. The idea is to later merge with the `buildings`, after first removing the rocks from the `best_people`\n",
        "\n",
        "### A note on Google Dynamic World\n",
        "It seems like we should also filter the certainty to be greater than 0.06 (highest false positive in the grassland had a certainty of 0.0569)."
      ],
      "metadata": {
        "id": "_2mu1gRR_64b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = 2022\n",
        "start_date = '{}-01-01'.format(year)\n",
        "end_date = '{}-01-01'.format(year + 1)\n",
        "\n",
        "# Google Dynamic World\n",
        "people = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filterDate(start_date, end_date).median().select('label').eq(6).selfMask()\n",
        "certainty_mask = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filterDate(start_date, end_date).median().select('built').gt(0.06).selfMask()\n",
        "best_people = people.mask(certainty_mask).eq(1).selfMask()\n",
        "\n",
        "# Google Open Buildings\n",
        "buildings = ee.FeatureCollection('GOOGLE/Research/open-buildings/v2/polygons').filter('confidence >= 0.70');\n",
        "\n",
        "buildings_raster = buildings.reduceToImage(\n",
        "  properties=['confidence'],\n",
        "  reducer=ee.Reducer.median()\n",
        ").gt(0).selfMask().select(['median'], ['label'])\n",
        "\n",
        "# Join with other layers\n",
        "built = best_people.unmask(0).add(buildings_raster.unmask(0)).gt(0).selfMask()"
      ],
      "metadata": {
        "id": "JsSrCRJq4UtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rasterise Google's Open Buildings"
      ],
      "metadata": {
        "id": "TgMXid3xA7dB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAR for rocks"
      ],
      "metadata": {
        "id": "M_t6j0AteVnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffa_db = ee.ImageCollection('COPERNICUS/S1_GRD').filterDate(ee.Date('2022-01-01'), ee.Date('2023-01-01')).filterBounds(aoi).mean()\n",
        "ffa_fl = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT').filterDate(ee.Date('2022-01-01'), ee.Date('2023-01-01')).filterBounds(aoi).mean()\n",
        "\n",
        "# Add VV-VH ratio\n",
        "def add_ratio(image):\n",
        "    ratio = image.select('VV').divide(image.select('VH')).rename('VV_VH_ratio')\n",
        "    return image.addBands(ratio)\n",
        "\n",
        "# Load Sentinel-1 data\n",
        "collection = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "                .filterBounds(aoi)\n",
        "                .filterDate('2022-01-01', '2023-01-01')\n",
        "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
        "                .filter(ee.Filter.eq('instrumentMode', 'IW'))).map(add_ratio)\n",
        "\n",
        "\n",
        "image = collection.mean()"
      ],
      "metadata": {
        "id": "CQJ3jAbMeY--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import training data"
      ],
      "metadata": {
        "id": "p_JRtJM-LHl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "filename = \"/content/drive/MyDrive/mygit/naturewatch_analysis/geometries/rocks.geojson\"\n",
        "\n",
        "# Load a GeoJSON file\n",
        "with open(filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert the GeoJSON into an ee.FeatureCollection\n",
        "fc = ee.FeatureCollection(data['features'])\n"
      ],
      "metadata": {
        "id": "6qOr3VKyLLca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 500\n",
        "classes = [0, 1]\n",
        "sample_list = []\n",
        "\n",
        "def addAttribute(feature, value):\n",
        "    return feature.set('label', value)\n",
        "\n",
        "for cls in classes:\n",
        "  polygons_class = fc.filter(ee.Filter.eq('label', cls))\n",
        "  randomPoints = ee.FeatureCollection.randomPoints(polygons_class, num_samples, 0, 1);\n",
        "  randomPoints = randomPoints.map(lambda feature: addAttribute(feature, cls))\n",
        "  sample_list.append(randomPoints)\n",
        "\n",
        "samples_all = sample_list[0].merge(sample_list[1])\n"
      ],
      "metadata": {
        "id": "LmNu6KdJP0lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for classification\n",
        "inputFeatures = ['VV', 'VH', 'VV_VH_ratio']\n",
        "\n",
        "# Extract band values for each training region\n",
        "samples = image.select(inputFeatures).sampleRegions(collection=samples_all, properties=['label'], scale=10)"
      ],
      "metadata": {
        "id": "ZyW1jAF5Omv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring VV and VH differences between rocks and buildings"
      ],
      "metadata": {
        "id": "cKcHsPMvPtJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate samples for each class\n",
        "samples_0 = samples.filter(ee.Filter.eq('label', 0))\n",
        "samples_1 = samples.filter(ee.Filter.eq('label', 1))\n",
        "\n",
        "band_oi = 'VV'\n",
        "\n",
        "# Min of each class\n",
        "mins_0 = samples_0.reduceColumns(reducer=ee.Reducer.min(), selectors=[band_oi])\n",
        "print('Min class 0: ', mins_0.getInfo())\n",
        "\n",
        "mins_1 = samples_1.reduceColumns(reducer=ee.Reducer.min(), selectors=[band_oi])\n",
        "print('Min class 1: ', mins_1.getInfo())\n",
        "\n",
        "# Max of each class\n",
        "max_0 = samples_0.reduceColumns(reducer=ee.Reducer.max(), selectors=[band_oi])\n",
        "print('Max class 0: ', max_0.getInfo())\n",
        "\n",
        "max_1 = samples_1.reduceColumns(reducer=ee.Reducer.max(), selectors=[band_oi])\n",
        "print('Max class 1: ', max_1.getInfo())\n",
        "\n",
        "# Mean of each class\n",
        "means_0 = samples_0.reduceColumns(reducer=ee.Reducer.mean(), selectors=[band_oi])\n",
        "print('Means class 0: ', means_0.getInfo())\n",
        "\n",
        "means_1 = samples_1.reduceColumns(reducer=ee.Reducer.mean(), selectors=[band_oi])\n",
        "print('Means class 1: ', means_1.getInfo())\n",
        "\n",
        "# Standard deviation of each class\n",
        "std_devs_0 = samples_0.reduceColumns(reducer=ee.Reducer.stdDev(), selectors=[band_oi])\n",
        "print('Standard deviations class 0: ', std_devs_0.getInfo())\n",
        "\n",
        "std_devs_1 = samples_1.reduceColumns(reducer=ee.Reducer.stdDev(), selectors=[band_oi])\n",
        "print('Standard deviations class 1: ', std_devs_1.getInfo())\n"
      ],
      "metadata": {
        "id": "KjxxLyMtPv6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exctract bands\n",
        "samples_info = samples.getInfo()\n",
        "VV_values = [feat['properties']['VV'] for feat in samples_info['features']]\n",
        "VH_values = [feat['properties']['VH'] for feat in samples_info['features']]\n",
        "VV_VH_ratio = [feat['properties']['VV_VH_ratio'] for feat in samples_info['features']]\n",
        "\n",
        "\n",
        "labels = [feat['properties']['label'] for feat in samples_info['features']]\n",
        "\n",
        "# Convert to pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'VV': VV_values,\n",
        "    'VH': VH_values,\n",
        "    'VV_VH_ratio': VV_VH_ratio,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "df_0 = df[df['label'] == 0]\n",
        "df_1 = df[df['label'] == 1]\n",
        "\n",
        "# Make histograms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.hist(df_0['VV'], bins=50, alpha=0.5, label='class 0')\n",
        "plt.hist(df_1['VV'], bins=50, alpha=0.5, label='class 1')\n",
        "plt.title('VV distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.hist(df_0['VH'], bins=50, alpha=0.5, label='class 0')\n",
        "plt.hist(df_1['VH'], bins=50, alpha=0.5, label='class 1')\n",
        "plt.title('VH distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.hist(df_0['VV_VH_ratio'], bins=50, alpha=0.5, label='class 0')\n",
        "plt.hist(df_1['VV_VH_ratio'], bins=50, alpha=0.5, label='class 1')\n",
        "plt.title('VV_VH_ratio')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(min(df_1['VV_VH_ratio']))\n"
      ],
      "metadata": {
        "id": "_D1xYRfdQypk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 1: Make average polygons\n",
        "\n",
        "1. Using the Google Dynamic World layer, we reduce all identified buildings to polygons.\n",
        "2. For each polygon, we then sample the maximum VV and VH\n",
        "3. We transform the polygons back to pixels, now with each pixel having a VV and VH value\n",
        "4. Finally, we build a random forest model, using our training data to sample this newly created raster"
      ],
      "metadata": {
        "id": "EuvfVK8Xdh55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# built_aoi = best_people.clipToCollection(aoi);\n",
        "\n",
        "# Convert to polygons\n",
        "built_polygons = best_people.reduceToVectors(\n",
        "  geometry=aoi,\n",
        "  crs=best_people.projection(),\n",
        "  scale=20,\n",
        "  geometryType='polygon',\n",
        "  eightConnected=False,\n",
        "  labelProperty='label',\n",
        "  maxPixels=10453920,\n",
        ");\n",
        "\n",
        "# Sample max VV and VH values\n",
        "poly_max =image.select(['VV', 'VH']).reduceRegions(\n",
        "    collection=built_polygons,\n",
        "    reducer=ee.Reducer.max(),\n",
        "    scale=10)\n",
        "\n",
        "img_max_VV = poly_max.reduceToImage(\n",
        "    properties = ['VV'],\n",
        "    reducer = ee.Reducer.first()\n",
        ").rename('VV')\n",
        "\n",
        "img_max_VH = poly_max.reduceToImage(\n",
        "    properties = ['VH'],\n",
        "    reducer = ee.Reducer.first()\n",
        ").rename('VH')\n",
        "\n",
        "img_max = img_max_VV.addBands(img_max_VH)\n",
        "\n",
        "# vv_rocks = vv_max.filter(ee.Filter.lt('max', -8))"
      ],
      "metadata": {
        "id": "0_ZzJ4YcdvWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a random forest model\n",
        "Making rasters of the polygons again, but assigning each pixel its polygon value and then building a classifier to predict the new rasters."
      ],
      "metadata": {
        "id": "vRu2ehJn8xR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for classification\n",
        "inputFeatures = ['VV', 'VH']\n",
        "\n",
        "# Extract band values for each training region\n",
        "samples = img_max.select(inputFeatures).sampleRegions(collection=samples_all, properties=['label'], scale=10)\n",
        "\n",
        "# Train the classifier\n",
        "classifier = ee.Classifier.smileRandomForest(numberOfTrees=50).train(features=samples, classProperty='label', inputProperties=inputFeatures)\n",
        "\n",
        "# Classify the images\n",
        "result = img_max.classify(classifier)"
      ],
      "metadata": {
        "id": "MSmyIAuc9dch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 2:  Neighbourhood operation\n",
        "Instead of making polygons, here we give each pixel the maximum value of all its surrounding pixels."
      ],
      "metadata": {
        "id": "sUCF78r9R8tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neighborhood\n",
        "kernel = ee.Kernel.square(3) # This will give a 3x3 neighborhood. Adjust as needed.\n",
        "\n",
        "# Apply the reducer function in the neighborhood\n",
        "max_kernel = image.reduceNeighborhood(**{\n",
        "    'reducer': ee.Reducer.max(),\n",
        "    'kernel': kernel,\n",
        "})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "21Ar7XFwSjSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for classification\n",
        "inputFeatures_kernel = ['VV_max', 'VH_max']\n",
        "\n",
        "# Extract band values for each training region\n",
        "samples_kernel = max_kernel.select(inputFeatures_kernel).sampleRegions(collection=samples_all, properties=['label'], scale=10)\n",
        "\n",
        "# Train the classifier\n",
        "classifier_kernel = ee.Classifier.smileRandomForest(numberOfTrees=50).train(features=samples_kernel, classProperty='label', inputProperties=inputFeatures_kernel)\n",
        "\n",
        "# Classify the images\n",
        "result_kernel = max_kernel.classify(classifier_kernel).mask(best_people)\n",
        "\n",
        "people_kernel = result_kernel.eq(0).selfMask()"
      ],
      "metadata": {
        "id": "GMquAXDKTsEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_kernel.bandNames().getInfo())"
      ],
      "metadata": {
        "id": "CSvjvoVwfbBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now convert to polygons and use a simple threshold to classify"
      ],
      "metadata": {
        "id": "sKokqhXce78t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to polygons\n",
        "kernel_polygons = result_kernel.reduceToVectors(\n",
        "  geometry=aoi,\n",
        "  crs=result_kernel.projection(),\n",
        "  scale=20,\n",
        "  geometryType='polygon',\n",
        "  eightConnected=False,\n",
        "  labelProperty='label',\n",
        "  maxPixels=10453920,\n",
        ")\n",
        "\n",
        "# Sample max VV and VH values\n",
        "poly_sum =result_kernel.select(['classification']).reduceRegions(\n",
        "    collection=kernel_polygons,\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    scale=10)\n",
        "\n",
        "poly_count =result_kernel.select(['classification']).reduceRegions(\n",
        "    collection=poly_sum,\n",
        "    reducer=ee.Reducer.count(),\n",
        "    scale=10)\n",
        "\n",
        "# Calculate ratio and round to two decimal places\n",
        "poly_ratio = poly_count.map(lambda feature:\n",
        "  feature.set('sum_count_ratio', ee.Number(feature.get('sum')).divide(ee.Number(feature.get('count'))).multiply(10000).round().divide(10000)))\n",
        "\n"
      ],
      "metadata": {
        "id": "77d8EKBFe56w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to raster\n",
        "\n",
        "kernel_class = poly_ratio.reduceToImage(\n",
        "    properties = ['sum_count_ratio'],\n",
        "    reducer = ee.Reducer.first()\n",
        ").rename('sum_count_ratio')\n"
      ],
      "metadata": {
        "id": "_gM7RO2JkRAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map"
      ],
      "metadata": {
        "id": "ABpfNq5p4xJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map.add_basemap('SATELLITE')\n",
        "\n",
        "# Map.addLayer(people, {'min':0, 'max':1, 'palette':['white','blue']}, 'people')\n",
        "# Map.addLayer(best_people, {'min':0, 'max':1, 'palette':['white','red']}, 'best_people')\n",
        "\n",
        "# Map.addLayer(buildings, {'color': 'red'}, 'Buildings confidence >= 0.70');\n",
        "# Map.addLayer(buildings_raster, {'min':0, 'max':1, 'palette':['white','red']}, 'buildings_raster')\n",
        "# Map.addLayer(gdw, {'min':0, 'max':8, 'palette':['419bdf', '397d49', '88b053', '7a87c6', 'e49635', 'dfc35a', 'c4281b', 'a59b8f', 'b39fe1']}, 'GDW')\n",
        "# Map.addLayer(people_c, {'min':0, 'max':0.1, 'palette':['white','blue']}, 'people_c')\n",
        "\n",
        "# Map.addLayer(built, {}, 'built')\n",
        "\n",
        "# Map.addLayer(ffa_db.select('VV'), {'min': -20, 'max': 0}, 'VV')\n",
        "# Map.addLayer(ffa_db.select('VH'), {'min': -20, 'max': 0}, 'VH')\n",
        "\n",
        "# Map.addLayer(rgb, {'min': [-20, -20, 0], 'max': [0, 0, 2]}, 'rgb')\n",
        "# Map.addLayer(result, {}, 'result')\n",
        "# Map.addLayer(samples, {}, 'fc')\n",
        "\n",
        "# Map.addLayer(img_max_VV, {'min': -14, 'max': -8, 'palette':['blue', 'white', 'red']}, 'vv_rocks') #-11\n",
        "# Map.addLayer(max_kernel.select('VV_max'), {'min': -14, 'max': -8, 'palette':['blue', 'white', 'red']}, 'VV_max') #-11\n",
        "\n",
        "# Map.addLayer(img_max_VH, {'min': -18, 'max': -12, 'palette':['blue', 'white', 'red']}, 'vh_rocks') #-15\n",
        "\n",
        "# Map.addLayer(bedrock, {}, 'bedrock')\n",
        "\n",
        "Map.addLayer(result, {'min':0, 'max':1, 'palette':['white','blue']}, 'result')\n",
        "# Map.addLayer(result_kernel, {'min':0, 'max':1, 'palette':['white','blue']}, 'result_kernel')\n",
        "# Map.addLayer(poly_ratio, {}, 'poly_count')\n",
        "# Map.addLayer(poly_count, {}, 'poly_count')\n",
        "\n",
        "# Map.addLayer(kernel_class, {'min':0, 'max':1, 'palette':['white','blue']}, 'kernel_class')\n",
        "\n",
        "Map.setCenter(31.273, -25.355, 16)\n",
        "Map\n"
      ],
      "metadata": {
        "id": "2tE6l2tE4zQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rUABCOGM3l0r"
      ],
      "authorship_tag": "ABX9TyOP72LFbKAj1Frw6n6zb+F1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}