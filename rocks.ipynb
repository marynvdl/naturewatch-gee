{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32MNb4_xsB7"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX91gkFHtj-5"
      },
      "outputs": [],
      "source": [
        "# Import and/or install libraries\n",
        "\n",
        "import subprocess, os\n",
        "\n",
        "try:\n",
        "    import geemap, ee\n",
        "except ImportError:\n",
        "    subprocess.check_call([\"python\", '-m', 'pip', 'install', '-U', 'geemap'])\n",
        "    import geemap, ee\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd4amEOkxAJ1"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Drive to access files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEtv-EFnvqgQ"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Earth Engine if neccessary\n",
        "\n",
        "service_account = os.environ.get('GOOGLE_SERVICE_ACCOUNT')\n",
        "credentials = ee.ServiceAccountCredentials(service_account, os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'))\n",
        "ee.Initialize(credentials)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Cloud\n",
        "\n",
        "from google.cloud import storage\n",
        "client = storage.Client()"
      ],
      "metadata": {
        "id": "BUzWkq9vbMin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started\n",
        "\n",
        "We will start by using Google Dynamic World and using the built layer. It seems like we should also filter the certainty to be greater than 0.06 (highest false positive in the grassland had a certainty of 0.0569)."
      ],
      "metadata": {
        "id": "rUABCOGM3l0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AOI\n",
        "\n",
        "aoi = ee.Geometry({\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              31.092023309844734,\n",
        "              -24.821258854952376\n",
        "            ],\n",
        "            [\n",
        "              31.092023309844734,\n",
        "              -25.303992405280965\n",
        "            ],\n",
        "            [\n",
        "              31.791037410539502,\n",
        "              -25.303992405280965\n",
        "            ],\n",
        "            [\n",
        "              31.791037410539502,\n",
        "              -24.821258854952376\n",
        "            ],\n",
        "            [\n",
        "              31.092023309844734,\n",
        "              -24.821258854952376\n",
        "            ]\n",
        "          ]\n",
        "        ],\n",
        "      })\n",
        "\n",
        "\n",
        "rock = ee.Geometry({\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              31.284,\n",
        "              -25.360\n",
        "            ],\n",
        "            [\n",
        "              31.285,\n",
        "              -25.360\n",
        "            ],\n",
        "            [\n",
        "              31.285,\n",
        "              -25.361\n",
        "            ],\n",
        "            [\n",
        "              31.284,\n",
        "              -25.361\n",
        "            ],\n",
        "            [\n",
        "              31.284,\n",
        "              -25.360\n",
        "            ],\n",
        "          ]\n",
        "        ],\n",
        "      })\n",
        "\n",
        "\n",
        "town = ee.Geometry({\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              31.14157858391627,\n",
        "              -25.17820869636482\n",
        "            ],\n",
        "            [\n",
        "              31.14157858391627,\n",
        "              -25.180385520009708\n",
        "            ],\n",
        "            [\n",
        "              31.143983964831108,\n",
        "              -25.180385520009708\n",
        "            ],\n",
        "            [\n",
        "              31.143983964831108,\n",
        "              -25.17820869636482\n",
        "            ],\n",
        "            [\n",
        "              31.14157858391627,\n",
        "              -25.17820869636482\n",
        "            ]\n",
        "          ]\n",
        "        ],\n",
        "      })"
      ],
      "metadata": {
        "id": "BfKUbBdm_eKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare layers\n",
        "\n",
        "First, we experiment with 2022 for our AOI."
      ],
      "metadata": {
        "id": "_2mu1gRR_64b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = 2022\n",
        "start_date = '{}-01-01'.format(year)\n",
        "end_date = '{}-01-01'.format(year + 1)\n",
        "\n",
        "# Google Dynamic World\n",
        "people = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filterDate(start_date, end_date).median().select('label').eq(6).selfMask()\n",
        "certainty_mask = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filterDate(start_date, end_date).median().select('built').gt(0.06).selfMask()\n",
        "best_people = people.mask(certainty_mask).eq(1).selfMask()\n",
        "\n",
        "# Google Open Buildings\n",
        "buildings = ee.FeatureCollection('GOOGLE/Research/open-buildings/v2/polygons').filter('confidence >= 0.70');\n",
        "\n",
        "buildings_raster = buildings.reduceToImage(\n",
        "  properties=['confidence'],\n",
        "  reducer=ee.Reducer.median()\n",
        ").gt(0).selfMask().select(['median'], ['label'])\n",
        "\n",
        "# Join with other layers\n",
        "built = best_people.unmask(0).add(buildings_raster.unmask(0)).gt(0).selfMask()"
      ],
      "metadata": {
        "id": "JsSrCRJq4UtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rasterise Google's Open Buildings"
      ],
      "metadata": {
        "id": "TgMXid3xA7dB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAR for rocks"
      ],
      "metadata": {
        "id": "M_t6j0AteVnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffa_db = ee.ImageCollection('COPERNICUS/S1_GRD').filterDate(ee.Date('2022-01-01'), ee.Date('2023-01-01')).filterBounds(aoi).mean()\n",
        "ffa_fl = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT').filterDate(ee.Date('2022-01-01'), ee.Date('2023-01-01')).filterBounds(aoi).mean()\n",
        "\n",
        "# Add VV-VH ratio\n",
        "def add_ratio(image):\n",
        "    ratio = image.select('VV').divide(image.select('VH')).rename('VV_VH_ratio')\n",
        "    return image.addBands(ratio)\n",
        "\n",
        "# Load Sentinel-1 data\n",
        "collection = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "                .filterBounds(aoi)\n",
        "                .filterDate('2022-01-01', '2023-01-01')\n",
        "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
        "                .filter(ee.Filter.eq('instrumentMode', 'IW'))).map(add_ratio)\n",
        "\n",
        "\n",
        "image = collection.mean()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CQJ3jAbMeY--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import training data"
      ],
      "metadata": {
        "id": "p_JRtJM-LHl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "filename = \"/content/drive/MyDrive/mygit/naturewatch_analysis/geometries/rocks.geojson\"\n",
        "\n",
        "# Load a GeoJSON file\n",
        "with open(filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert the GeoJSON into an ee.FeatureCollection\n",
        "fc = ee.FeatureCollection(data['features'])\n"
      ],
      "metadata": {
        "id": "6qOr3VKyLLca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 500\n",
        "classes = [0, 1]\n",
        "sample_list = []\n",
        "\n",
        "def addAttribute(feature, value):\n",
        "    return feature.set('label', value)\n",
        "\n",
        "for cls in classes:\n",
        "  polygons_class = fc.filter(ee.Filter.eq('label', cls))\n",
        "  randomPoints = ee.FeatureCollection.randomPoints(polygons_class, num_samples, 0, 1);\n",
        "  randomPoints = randomPoints.map(lambda feature: addAttribute(feature, cls))\n",
        "  sample_list.append(randomPoints)\n",
        "\n",
        "samples_all = sample_list[0].merge(sample_list[1])\n"
      ],
      "metadata": {
        "id": "LmNu6KdJP0lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for classification\n",
        "inputFeatures = ['VV', 'VH', 'VV_VH_ratio']\n",
        "\n",
        "# Extract band values for each training region\n",
        "samples = image.select(inputFeatures).sampleRegions(collection=samples_all, properties=['label'], scale=10)\n",
        "\n",
        "# Train the classifier\n",
        "classifier = ee.Classifier.smileRandomForest(numberOfTrees=50).train(features=samples, classProperty='label', inputProperties=inputFeatures)\n",
        "\n",
        "# Classify the images\n",
        "result = image.classify(classifier)"
      ],
      "metadata": {
        "id": "1zMKMtNhMywC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary statistics"
      ],
      "metadata": {
        "id": "cKcHsPMvPtJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate samples for each class\n",
        "samples_0 = samples.filter(ee.Filter.eq('label', 0))\n",
        "samples_1 = samples.filter(ee.Filter.eq('label', 1))\n",
        "\n",
        "band_oi = 'VV'\n",
        "\n",
        "# Min of each class\n",
        "mins_0 = samples_0.reduceColumns(reducer=ee.Reducer.min(), selectors=[band_oi])\n",
        "print('Min class 0: ', mins_0.getInfo())\n",
        "\n",
        "mins_1 = samples_1.reduceColumns(reducer=ee.Reducer.min(), selectors=[band_oi])\n",
        "print('Min class 1: ', mins_1.getInfo())\n",
        "\n",
        "# Max of each class\n",
        "max_0 = samples_0.reduceColumns(reducer=ee.Reducer.max(), selectors=[band_oi])\n",
        "print('Max class 0: ', max_0.getInfo())\n",
        "\n",
        "max_1 = samples_1.reduceColumns(reducer=ee.Reducer.max(), selectors=[band_oi])\n",
        "print('Max class 1: ', max_1.getInfo())\n",
        "\n",
        "# Mean of each class\n",
        "means_0 = samples_0.reduceColumns(reducer=ee.Reducer.mean(), selectors=[band_oi])\n",
        "print('Means class 0: ', means_0.getInfo())\n",
        "\n",
        "means_1 = samples_1.reduceColumns(reducer=ee.Reducer.mean(), selectors=[band_oi])\n",
        "print('Means class 1: ', means_1.getInfo())\n",
        "\n",
        "# Standard deviation of each class\n",
        "std_devs_0 = samples_0.reduceColumns(reducer=ee.Reducer.stdDev(), selectors=[band_oi])\n",
        "print('Standard deviations class 0: ', std_devs_0.getInfo())\n",
        "\n",
        "std_devs_1 = samples_1.reduceColumns(reducer=ee.Reducer.stdDev(), selectors=[band_oi])\n",
        "print('Standard deviations class 1: ', std_devs_1.getInfo())\n"
      ],
      "metadata": {
        "id": "KjxxLyMtPv6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exctract bands\n",
        "samples_info = samples.getInfo()\n",
        "VV_values = [feat['properties']['VV'] for feat in samples_info['features']]\n",
        "VH_values = [feat['properties']['VH'] for feat in samples_info['features']]\n",
        "VV_VH_ratio = [feat['properties']['VV_VH_ratio'] for feat in samples_info['features']]\n",
        "\n",
        "\n",
        "labels = [feat['properties']['label'] for feat in samples_info['features']]\n",
        "\n",
        "# Convert to pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'VV': VV_values,\n",
        "    'VH': VH_values,\n",
        "    'VV_VH_ratio': VV_VH_ratio,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "df_0 = df[df['label'] == 0]\n",
        "df_1 = df[df['label'] == 1]\n",
        "\n",
        "# Make histograms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.hist(df_0['VV'], bins=50, alpha=0.5, label='class 0')\n",
        "plt.hist(df_1['VV'], bins=50, alpha=0.5, label='class 1')\n",
        "plt.title('VV distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.hist(df_0['VH'], bins=50, alpha=0.5, label='class 0')\n",
        "plt.hist(df_1['VH'], bins=50, alpha=0.5, label='class 1')\n",
        "plt.title('VH distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.hist(df_0['VV_VH_ratio'], bins=50, alpha=0.5, label='class 0')\n",
        "plt.hist(df_1['VV_VH_ratio'], bins=50, alpha=0.5, label='class 1')\n",
        "plt.title('VV_VH_ratio')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(min(df_1['VV_VH_ratio']))\n"
      ],
      "metadata": {
        "id": "_D1xYRfdQypk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make polygons of built"
      ],
      "metadata": {
        "id": "EuvfVK8Xdh55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# built_aoi = best_people.clipToCollection(aoi);\n",
        "\n",
        "# Convert to polygons\n",
        "built_polygons = best_people.reduceToVectors(\n",
        "  geometry=aoi,\n",
        "  crs=best_people.projection(),\n",
        "  scale=20,\n",
        "  geometryType='polygon',\n",
        "  eightConnected=False,\n",
        "  labelProperty='label',\n",
        "  maxPixels=10453920,\n",
        ");\n",
        "\n",
        "# Sample max VV and VH values\n",
        "poly_max =image.select(['VV', 'VH']).reduceRegions(\n",
        "    collection=built_polygons,\n",
        "    reducer=ee.Reducer.max(),\n",
        "    scale=10)\n",
        "\n",
        "img_max_VV = poly_max.reduceToImage(\n",
        "    properties = ['VV'],\n",
        "    reducer = ee.Reducer.first()\n",
        ").rename('VV')\n",
        "\n",
        "img_max_VH = poly_max.reduceToImage(\n",
        "    properties = ['VH'],\n",
        "    reducer = ee.Reducer.first()\n",
        ").rename('VH')\n",
        "\n",
        "img_max = img_max_VV.addBands(img_max_VH)\n",
        "\n",
        "# vv_rocks = vv_max.filter(ee.Filter.lt('max', -8))"
      ],
      "metadata": {
        "id": "0_ZzJ4YcdvWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a random forest model\n",
        "Making rasters of the polygons again, but assigning each pixel its polygon value and then building a classifier to predict the new rasters."
      ],
      "metadata": {
        "id": "vRu2ehJn8xR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for classification\n",
        "inputFeatures = ['VV', 'VH']\n",
        "\n",
        "# Extract band values for each training region\n",
        "samples = img_max.select(inputFeatures).sampleRegions(collection=samples_all, properties=['label'], scale=10)\n",
        "\n",
        "# Train the classifier\n",
        "classifier = ee.Classifier.smileRandomForest(numberOfTrees=50).train(features=samples, classProperty='label', inputProperties=inputFeatures)\n",
        "\n",
        "# Classify the images\n",
        "result = img_max.classify(classifier)"
      ],
      "metadata": {
        "id": "MSmyIAuc9dch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map"
      ],
      "metadata": {
        "id": "ABpfNq5p4xJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map.add_basemap('SATELLITE')\n",
        "\n",
        "# Map.addLayer(people, {'min':0, 'max':1, 'palette':['white','blue']}, 'people')\n",
        "# Map.addLayer(best_people, {'min':0, 'max':1, 'palette':['white','red']}, 'best_people')\n",
        "\n",
        "# Map.addLayer(buildings, {'color': 'red'}, 'Buildings confidence >= 0.70');\n",
        "# Map.addLayer(buildings_raster, {'min':0, 'max':1, 'palette':['white','red']}, 'buildings_raster')\n",
        "# Map.addLayer(gdw, {'min':0, 'max':8, 'palette':['419bdf', '397d49', '88b053', '7a87c6', 'e49635', 'dfc35a', 'c4281b', 'a59b8f', 'b39fe1']}, 'GDW')\n",
        "# Map.addLayer(people_c, {'min':0, 'max':0.1, 'palette':['white','blue']}, 'people_c')\n",
        "\n",
        "# Map.addLayer(built, {}, 'built')\n",
        "\n",
        "# Map.addLayer(ffa_db.select('VV'), {'min': -20, 'max': 0}, 'VV')\n",
        "# Map.addLayer(ffa_db.select('VH'), {'min': -20, 'max': 0}, 'VH')\n",
        "\n",
        "# Map.addLayer(rgb, {'min': [-20, -20, 0], 'max': [0, 0, 2]}, 'rgb')\n",
        "# Map.addLayer(result, {}, 'result')\n",
        "# Map.addLayer(samples, {}, 'fc')\n",
        "\n",
        "Map.addLayer(img_max_VV, {'min': -14, 'max': -8, 'palette':['blue', 'white', 'red']}, 'vv_rocks') #-11\n",
        "# Map.addLayer(img_max_VH, {'min': -18, 'max': -12, 'palette':['blue', 'white', 'red']}, 'vh_rocks') #-15\n",
        "\n",
        "# Map.addLayer(bedrock, {}, 'bedrock')\n",
        "\n",
        "Map.addLayer(result, {'min':0, 'max':1, 'palette':['white','blue']}, 'result')\n",
        "\n",
        "Map.setCenter(31.273, -25.355, 16)\n",
        "Map\n"
      ],
      "metadata": {
        "id": "2tE6l2tE4zQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rUABCOGM3l0r",
        "_2mu1gRR_64b"
      ],
      "authorship_tag": "ABX9TyNvsh6fPYvhhD9UxUdPeRKF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}